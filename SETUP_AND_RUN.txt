================================================================================
ControlSpeech + Mamba TTS - Setup Commands
================================================================================

# 1. Create conda environment
conda env create -f environment.yml
conda activate mambatts

# 2. Install additional packages
pip install mamba-ssm textgrid

# 3. Run setup script (downloads ~90GB, extracts audio, pairs transcripts)
#    This will take 30-60 minutes depending on internet speed
bash setup.sh

# 4. Install and run MFA (Montreal Forced Aligner)
conda install -c conda-forge montreal-forced-aligner
mfa model download acoustic english_mfa
mfa model download dictionary english_mfa

# Run MFA alignment (this will take several hours for 289k samples)
mfa align \
    VccmDataset/audio_extracted \
    english_mfa \
    english_mfa \
    VccmDataset/mfa_outputs

# 5. Preprocess dataset
#    NOTE: Use --audio_dir (extracted files) instead of --tarball
#    The tarball only contains ~56k files, but audio_extracted has all 418k files

# Recommended: Parallel preprocessing (faster)
python -m data_utils.preprocess_parallel \
    --csv_path VccmDataset/controlspeech_train.csv \
    --output_dir processed_data/ \
    --audio_dir VccmDataset/audio_extracted \
    --phoneme_vocab_path . \
    --cpu_workers 6 \
    --gpu_batch_size 32 \
    --io_workers 4

# Alternative: Serial preprocessing (slower, simpler)
python -m data_utils.preprocess \
    --csv_path VccmDataset/controlspeech_train.csv \
    --output_dir processed_data/ \
    --audio_dir VccmDataset/audio_extracted \
    --phoneme_vocab_path .

# 6. Train
python train.py --config config.yaml

# 7. Resume training (if interrupted)
python train.py --config config.yaml --resume checkpoints/last_checkpoint.pt

================================================================================
Dataset Summary (after preprocessing completes):
================================================================================
- Total audio files: ~418k WAV files
- Samples with transcripts: ~289k (matches CSV entries)
- Emotional datasets: ESD, MEAD, CREMA-D, TESS, RAVDESS, SAVEE, MESS (~42k)
- LibriTTS (non-emotional): ~247k samples
- Total download size: ~90GB
- Extracted size: ~110GB

Preprocessed Output (processed_data/):
- metadata.json: 633 MB (289,104 samples)
- tensors/: 17 GB (867,312 files = 3 per sample)
  - {item_name}_phonemes.pt: Phoneme token IDs
  - {item_name}_style.pt: 512-dim BERT style embeddings
  - {item_name}_codec.pt: FACodec tokens (T, 6)

MFA Alignments (VccmDataset/mfa_outputs/):
- 288,775 TextGrid files in nested directory structure
- Training automatically loads durations from TextGrids
- Samples without matching TextGrids are skipped (require_mfa=true in config)
================================================================================
NOTE: Training uses preprocessed tensors (no on-the-fly FACodec encoding).
This is much faster than the original approach. MFA alignments are still
loaded on-the-fly from TextGrid files during training.
================================================================================
