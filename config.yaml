# ControlSpeech + Mamba TTS Training Configuration
# Based on ControlSpeech paper (arXiv:2406.01205) and baseline configs

# Training hyperparameters
training:
  batch_size: 4  # Reduced from 32 due to OOM (long sequences: 6144 tokens)
  max_steps: 400000  # Full training (use 10 for sanity check)
  
  # Optimizer settings (from baseline configs)
  lr: 0.0002  # 2e-4 (baseline PromptStyle)
  betas: [0.8, 0.99]  # Changed from default [0.9, 0.999]
  eps: 1.0e-9  # Changed from default 1e-8
  weight_decay: 0.0
  
  # Learning rate schedule
  warmup_steps: 800  # Warmup period
  lr_decay: 0.999875  # Exponential decay per step
  
  # Gradient clipping
  clip_grad_norm: 1.0
  
  # Loss weights (from ControlSpeech baseline)
  w_codec: 1.0  # Codec reconstruction (main loss)
  w_dur: 1.0  # Duration prediction (matches baseline)
  w_smsd: 0.5  # Style mixture density loss
  
  # Logging and Checkpointing
  log_interval: 100
  save_interval: 5000  # Validate and save best model every N steps
  save_last_checkpoint: true  # Keep last checkpoint for crash recovery
  
  # Early Stopping (optional - set patience to 0 to disable)
  early_stopping_patience: 10  # Stop if no improvement for N validation checks (0 = disabled)
  early_stopping_min_delta: 0.001  # Minimum improvement to count as better
  
  # Device
  device: cuda  # "cuda" or "cpu" (auto-detect in code)
  seed: 1234

# Model architecture - CONFIGURABLE hyperparameters only
# (Architecture constants are hardcoded in code)
model:
  # These are defined as constants in train.py but referenced here for documentation
  d_model: 512
  d_style: 512
  vocab_size_audio: 1024
  num_quantizers: 6
  
  # Text Encoder (FastSpeech2-style FFT blocks)
  text_encoder:
    n_layers: 4  # Number of FFT blocks (baseline: 4, can experiment)
    dropout: 0.1
  
  # Duration Predictor
  duration_predictor:
    dropout: 0.1
  
  # SMSD (Style Mixture Semantic Density)
  smsd:
    num_mixtures: 5  # K Gaussian components (paper tested: 3, 5, 7)
    dropout: 0.1
    variance_mode: "isotropic_across_clusters"  # Paper tested different modes
  
  # Style Conditioning Pipeline
  style_conditioning:
    dropout: 0.1
  
  # Mamba Decoder (MAVE architecture)
  mamba_decoder:
    n_layers: 6  # Number of Mamba layers (per ControlSpeech Appendix F: 6)
    max_len: 8192  # Max sequence length for positional encoding

# Data configuration - Uses PREPROCESSED tensors from processed_data/
data:
  # Preprocessed data directory (from preprocess_parallel.py output)
  processed_dir: "processed_data"
  
  # MFA alignments directory (nested structure with TextGrid files)
  mfa_root: "VccmDataset/mfa_outputs"
  
  # Whether to require MFA alignments (skip samples without them)
  require_mfa: true
  
  # Validation split (for best model selection and early stopping)
  validation_split: 0.01  # Use 1% of data for validation (~2,890 samples)
  
  # DataLoader settings
  num_workers: 4  # Number of data loading workers
  shuffle: true
  pin_memory: true

# Paths
paths:
  phoneme_vocab: "phoneme_vocab.json"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  output_dir: "outputs"
